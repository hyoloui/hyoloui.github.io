---
sidebar_position: 5
title: 사이킷런을 통한 머신러닝 1
description: 머신러닝 프로젝트의 표준 처리 절차와 파이썬 라이브러리인 사이킷런(Scikit-learn)을 활용한 핵심 알고리즘 구현 방법을 종합적으로 학습
---

머신러닝 프로젝트의 표준 처리 절차와 파이썬 라이브러리인 사이킷런(Scikit-learn)을 활용한 핵심 알고리즘 구현 방법을 종합적으로 학습한다.<br/>
머신러닝의 전체 파이프라인은 **데이터 수집, 데이터 전처리, 데이터 학습, 모델 평가**의 4단계로 구성되며,<br/>
각 단계는 모델의 성능을 결정하는 데 중요한 역할을 한다.

데이터 수집 단계에서는 학습, 검증, 평가 데이터셋을 정의하고, 일반적으로 7:3 또는 8:2 비율로 데이터를 분할하여 모델의 신뢰도를 확보한다.

데이터 전처리 단계는 누락 데이터 처리, 텍스트 데이터의 수치화, 특성 스케일링(Feature Scaling) 등<br/>
머신러닝 알고리즘이 효과적으로 학습할 수 있도록 데이터를 정제하고 변환하는 핵심 과정이다.

데이터 학습 단계에서는 **지도 학습, 비지도 학습, 강화 학습** 등 문제의 성격에 맞는 학습 방법과 알고리즘을 선택한다.<br/>
사이킷런은 **선형 회귀, 로지스틱 회귀, 의사결정 트리, 서포트 벡터 머신(SVM), 랜덤 포레스트** 등 다양한 지도 및 비지도 학습 알고리즘을 표준화된 API로 제공하여 효율적인 모델 개발을 지원한다.

마지막으로, 모델 평가 단계에서는 **정확도(Accuracy), 정밀도(Precision), 재현율(Recall)** 과 같은 지표를 사용하여 학습된 모델의 성능을 객관적으로 측정하고 검증한다.<br/>
이러한 전 과정에 대한 상세한 설명과 사이킷런을 통한 구체적인 구현 방법을 제시하여 머신러닝 프로젝트 수행에 대한 깊이 있는 이해를 제공한다.

---

## 1. 머신러닝의 표준 처리 절차

머신러닝 프로젝트는 일반적으로 데이터 수집부터 모델 평가에 이르는 체계적인 4단계 절차를 따른다.

### 1.1. 데이터 수집 (Data Collection)

데이터 수집은 머신러닝 모델의 기반을 마련하는 첫 단계로, 양질의 데이터를 확보하고 목적에 맞게 분할하는 과정이 포함된다.

- 데이터셋의 종류:
  - 학습 데이터셋 (Training Set): 머신러닝 알고리즘을 학습시키는 데 사용되는 데이터.
  - 검증 데이터셋 (Validation Set): 학습된 모델의 예측 정확도를 계산하고 하이퍼파라미터 튜닝에 사용되는 데이터.
  - 평가 데이터셋 (Test Set): 모델 구축 및 튜닝 과정에 사용되지 않은, 실제 데이터와 유사한 데이터로 최종 성능을 평가하는 데 사용.
- 데이터셋 분할:
  - 학습 데이터가 너무 적으면 알고리즘이 효율적으로 학습하기 어렵다.
  - 검증 데이터가 너무 적으면 정확도, 정밀도, 재현율의 신뢰도가 낮아진다.
  - 효과적인 분할 비율은 **학습 데이터 : 검증 데이터 = 80%:20% 또는 70%:30%** 이다.
- 대표적인 데이터셋 수집 사이트:
  | 사이트명 | URL | 주요 특징 |
  | -------------------- | ------------------------------------------ | ---------------------------------------------------- |
  | 네이버 데이터랩 | http://datalab.naver.com/ | 국내 검색 트렌드 및 쇼핑 데이터 제공 |
  | 공공데이터 포털 | https://www.data.go.kr/ | 대한민국 정부가 제공하는 공공 데이터 |
  | K-ICT 빅데이터 센터 | https://kbig.kr/#none | 빅데이터 분석 동향 및 실습 데이터 제공 |
  | 건강보험심사평가원 | https://www.hira.or.kr/ | 의료 관련 통계 데이터 |
  | Kaggle | https://www.kaggle.com/ | 전 세계 최대 데이터 과학 경진대회 및 데이터셋 플랫폼 |
  | Google Datasets | https://datasetsearch.research.google.com/ | 구글에서 제공하는 데이터셋 검색 엔진 |

### 1.2. 데이터 전처리 (Data Preprocessing)

데이터 전처리는 수집된 원시 데이터를 머신러닝 알고리즘에 적합한 형태로 변환하는 과정이다.

- 주요 전처리 기법:
  - Data Cleaning: 누락되거나 불필요한 데이터를 제거(drop(), dropna())하거나 특정 값으로 채운다(fillna()).
  - Handling Text and Categorical Attributes: 대부분의 알고리즘은 수치 데이터를 기반으로 하므로, 텍스트 데이터를 수치로 인코딩한다 (factorize(), OrdinalEncoder(), OneHotEncoder()).
  - Custom Transformers: 사이킷런의 데이터 변환기(fit(), transform(), fit_transform())를 활용하여 데이터를 변환한다.
  - Feature Scaling: 데이터의 범위를 조정하여 모델이 안정적으로 학습하도록 돕는다.
    - 정규화 (Normalization): 데이터 범위를 0과 1 사이로 조정 (Min-Max Scaling).
    - 표준화 (Standardization): 데이터의 평균을 0, 표준편차를 1로 조정.
  - Transformation Pipelines: 반복적인 전처리 과정을 파이프라인으로 구성하여 작업의 효율성을 높인다.
- 결측치 처리:
  - 데이터가 충분할 경우: 결측치가 포함된 행 또는 열을 제거한다.
  - 데이터가 부족할 경우: 평균, 중앙값, 최빈값 등 통계적 방법으로 결측치를 채운다.

### 1.3. 데이터 학습 (Data Training)

전처리된 데이터를 사용하여 모델을 학습시키는 단계로, 문제의 종류에 따라 적절한 학습 방법과 알고리즘을 선택해야 한다.

- 학습 방법의 종류:

  - 지도 학습 (Supervised Learning): 정답(레이블)이 있는 데이터를 사용하여 모델을 학습.
    - 분류 (Classification): 데이터를 정해진 카테고리로 구분 (예: SVM, 의사결정 트리, 나이브 베이즈).
    - 회귀 (Regression): 연속적인 값을 예측 (예: 선형 회귀).
  - 비지도 학습 (Unsupervised Learning): 정답 없이 데이터 자체의 패턴이나 구조를 학습.
    - 군집화 (Clustering): 유사한 데이터끼리 그룹화 (예: 가우시안 믹스쳐 모델).
  - 강화 학습 (Reinforcement Learning): 보상을 최대화하는 방향으로 행동을 학습.

### 1.4. 모델 평가 (Model Evaluation)

학습된 모델의 성능을 객관적인 지표로 측정하는 단계이다.

- 혼동 행렬 (Confusion Matrix): 모델의 예측 성능을 시각적으로 나타내는 표.

  |                | 예측: Positive      | 예측: Negative      |
  | -------------- | ------------------- | ------------------- |
  | 실제: Positive | TP (True Positive)  | FN (False Negative) |
  | 실제: Negative | FP (False Positive) | TN (True Negative)  |

- 주요 평가 지표:
  - 정확도 (Accuracy): 전체 예측 중 올바르게 예측한 비율.
    - (TP + TN) / (TP + TN + FP + FN)
  - 재현율 (Recall): 실제 Positive인 것들 중 모델이 Positive라고 올바르게 예측한 비율.
    - TP / (FN + TP)
  - 정밀도 (Precision): 모델이 Positive라고 예측한 것들 중 실제로 Positive인 것의 비율.
    - TP / (TP + FP)

## 2. 사이킷런(Scikit-learn)을 활용한 머신러닝 구현

사이킷런은 파이썬에서 가장 널리 사용되는 머신러닝 라이브러리로, 다양한 알고리즘과 데이터 처리 도구를 제공한다.

### 2.1. 사이킷런 개요 및 핵심 프로세스

- 개요: 지도/비지도 학습 알고리즘을 포함한 다양한 머신러닝 기능을 제공하는 파이썬 라이브러리.
- 설치: pip install scikit-learn (구글 코랩에서는 기본 제공)
- 처리 과정:

  1. 라이브러리 Import
  2. 데이터 로드 (Load)
  3. 학습/테스트 데이터 분할 (Split)
  4. 알고리즘에 따른 모델 생성
  5. 모델 학습 (fit)
  6. 예측 (predict)
  7. 성능 평가 (score)

### 2.2. 주요 라이브러리 및 내장 데이터

- 대표적인 라이브러리 Import:
  - 내장 데이터셋: from sklearn.datasets import load_iris
  - 데이터 분할: from sklearn.model_selection import train_test_split
  - 알고리즘: from sklearn.tree import DecisionTreeClassifier
  - 성능 평가: from sklearn.metrics import accuracy_score
- 내장 데이터셋:
  - 사이킷런은 sklearn.utils.Bunch라는 Key-Value 형태의 자료구조로 데이터를 제공한다.
  - 주요 Key: data(샘플 데이터), target(레이블), feature_names, target_names, DESCR(설명).

| 함수명               | 용도 | 설명                    |
| -------------------- | ---- | ----------------------- |
| load_boston()        | 회귀 | 보스턴 집값 데이터      |
| load_iris()          | 분류 | 붓꽃 품종 데이터        |
| load_diabetes()      | 회귀 | 당뇨병 데이터           |
| load_digits()        | 분류 | 숫자 이미지 픽셀 데이터 |
| load_wine()          | 분류 | 와인 등급 데이터        |
| load_breast_cancer() | 분류 | 위스콘신 유방암 데이터  |

### 2.3. 가상 데이터 생성 도구

사이킷런은 알고리즘 테스트를 위해 다양한 가상 분류용 데이터를 생성하는 함수를 제공한다.

| 함수명                  | 라이브러리       | 주요 역할                                       |
| ----------------------- | ---------------- | ----------------------------------------------- |
| make_classification     | sklearn.datasets | 일반적인 가상 분류 모형 데이터 생성             |
| make_blobs              | sklearn.datasets | 등방성 가우시안 정규분포를 이용한 클러스터 생성 |
| make_moons              | sklearn.datasets | 초승달 모양의 비선형 클러스터 2개 생성          |
| make_gaussian_quantiles | sklearn.datasets | 다차원 가우시안 분포를 이용해 클래스 분류       |

### 2.4. 데이터 분할 (train_test_split)

`from sklearn.model_selection import train_test_split`를 사용하여 데이터를 학습용과 테스트용으로 분할한다.

- 주요 파라미터:
  - test_size: 테스트 데이터셋의 비율 (예: 0.2)
  - train_size: 학습 데이터셋의 비율 (지정하지 않으면 1 - test_size)
  - random_state: 재현 가능한 결과를 위해 사용하는 난수 시드

## 3. 주요 머신러닝 알고리즘 상세 분석

사이킷런으로 구현 가능한 주요 지도 학습 알고리즘은 다음과 같다.

### 3.1. 선형 회귀 (Linear Regression)

- 목적: 종속 변수(y)와 하나 이상의 독립 변수(x) 간의 선형 상관 관계를 모델링하여 값을 예측.
- 라이브러리: `from sklearn.linear_model import LinearRegression`

### 3.2. 로지스틱 회귀 (Logistic Regression)

- 목적: 특정 사건이 발생할 확률을 예측하는 모델로, 데이터가 어떤 범주에 속할 확률을 0과 1 사이의 값으로 예측.
- 라이브러리: `from sklearn.linear_model import LogisticRegression`

### 3.3. K-최근접 이웃 (KNN)

- 목적: 새로운 데이터 포인트에 대해 기존 훈련 데이터셋에서 가장 가까운 K개의 이웃을 찾아 분류 또는 회귀를 수행하는 간단한 알고리즘.
- 라이브러리: `from sklearn.neighbors import KNeighborsClassifier`

### 3.4. 나이브 베이즈 (Naive Bayes)

- 목적: 각 특성이 독립적이라고 가정하고 베이즈 정리를 적용하여 데이터를 간단하게 분류.
- 종류 및 라이브러리:
  - 가우시안 (GaussianNB): 연속적인 데이터에 사용 (`from sklearn.naive_bayes import GaussianNB`).
  - 다항분포 (MultinomialNB): 카운트 기반 데이터에 사용 (`from sklearn.naive_bayes import MultinomialNB`).
  - 베르누이 (BernoulliNB): 이진 데이터에 사용 (`from sklearn.naive_bayes import BernoulliNB`).

### 3.5. 의사결정 트리 (Decision Tree)

- 목적: 데이터의 속성을 분석하여 패턴을 찾고, 이를 기반으로 분류나 회귀를 수행하는 트리 구조의 알고리즘.
- 특징:
  - Greedy 방식: 각 분기점에서 최적의 선택만을 고려하므로 전역 최적해를 보장하지 못할 수 있음.
  - 오버피팅(과적합) 경향이 있어 가지치기(Pruning)가 필요함.
- 핵심 개념:
  - 지니 불순도 (Gini Impurity): 한 노드에 여러 클래스의 데이터가 얼마나 섞여 있는지를 나타내는 지표. 낮을수록 순수함.
  - 정보 획득량 (Information Gain): 특정 질문(분기)을 통해 불순도가 얼마나 감소했는지를 측정. 정보 획득량이 큰 순서대로 트리를 구성.
- 라이브러리:
  - 분류: `from sklearn.tree import DecisionTreeClassifier`
  - 회귀: `from sklearn.tree import DecisionTreeRegressor`

### 3.6. 서포트 벡터 머신 (SVM)

- 목적: 데이터를 분류하기 위한 강력한 지도 학습 모델로, 클래스 간의 경계(Decision Boundary)를 정의.
- 핵심 개념:
  - 서포트 벡터 (Support Vector): 결정 경계와 가장 가까운 데이터 포인트.
  - 마진 (Margin): 결정 경계와 서포트 벡터 사이의 거리. SVM은 이 마진을 최대화하는 것을 목표로 함.
  - 하드 마진 vs. 소프트 마진: 이상치(Outlier)를 허용하지 않는 하드 마진은 오버피팅을, 어느 정도 허용하는 소프트 마진은 언더피팅의 위험이 있음.
- 라이브러리: `from sklearn import svm` (예: `svm.SVC()`)

### 3.7. 랜덤 포레스트 (Random Forest)

- 목적: 여러 개의 의사결정 트리를 생성하고 그 결과를 종합(앙상블)하여 단일 트리의 오버피팅 문제를 해결하는 모델.
- 핵심 개념:
  - 배깅 (Bagging): 원본 훈련 데이터에서 중복을 허용하여 여러 개의 서브 데이터셋을 만들고, 각 서브셋으로 개별 트리를 학습시키는 기법.
  - 속성 제한: 각 트리를 만들 때 전체 특성 중 일부만 무작위로 선택하여 사용함으로써 트리의 다양성을 확보.
- 라이브러리: `from sklearn.ensemble import RandomForestClassifier`
